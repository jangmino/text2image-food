{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import webdataset as wds\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = r\"/home/work/llm_data/datasets/food-images/Training\"\n",
    "labels_dir = r\"/home/work/llm_data/datasets/food-images/Labels\"\n",
    "meta_file = r\"/home/work/llm_data/datasets/food-images/metadata.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dirs = list(os.listdir(training_dir))\n",
    "label_dirs = list(os.listdir(labels_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_folders = glob.glob(\"/home/work/llm_data/datasets/Training/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(all_image_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_label_folders = glob.glob(\"/home/work/llm_data/datasets/Labels/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ll = normalize_folder_name(os.path.basename(sorted(all_label_folders)[103]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ll)):\n",
    "    print(ll[i], tt[i], ll[i] == tt[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글-영어 음식명 딕셔너리 생성\n",
    "name_dict = {}\n",
    "with open('name_dict.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(', ')\n",
    "        if len(parts) == 3:\n",
    "            _, korean, english = parts\n",
    "            name_dict[korean] = english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_folder_name(name):\n",
    "    # 'json' 접미사 제거 및 앞뒤 공백 제거\n",
    "    return name.replace(' json', '').strip().replace('.json', '')\n",
    "\n",
    "# 데이터프레임 생성을 위한 리스트\n",
    "data = []\n",
    "\n",
    "# Training 폴더의 경로를 기준으로 매칭\n",
    "for train_dir in all_image_folders:\n",
    "    text = os.path.basename(train_dir)\n",
    "    normalized_text = normalize_folder_name(text)\n",
    "    \n",
    "    # Labels 폴더에서 매칭되는 경로 찾기\n",
    "    matching_label_dir = next(\n",
    "        (label_dir for label_dir in all_label_folders \n",
    "         if normalize_folder_name(os.path.basename(label_dir)) == normalized_text),\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    if matching_label_dir:\n",
    "        data.append({\n",
    "            'han_text': text,\n",
    "            'train_dir': train_dir,\n",
    "            'label_dir': matching_label_dir\n",
    "        })\n",
    "    else:\n",
    "        print(f'No matching label for {text}, {normalized_text}, {train_dir}')\n",
    "        # break\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df.han_text.map(name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(row):\n",
    "    results = []\n",
    "    \n",
    "    # train_dir에서 jpg 파일 목록 가져오기\n",
    "    jpg_files = [f for f in os.listdir(row['train_dir']) if f.endswith('.jpg')]\n",
    "    \n",
    "    for jpg_file in jpg_files:\n",
    "        jpg_path = os.path.join(row['train_dir'], jpg_file)\n",
    "        json_file = os.path.splitext(jpg_file)[0] + '.json'\n",
    "        json_path = os.path.join(row['label_dir'], json_file)\n",
    "        \n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            if json_data and isinstance(json_data, list):\n",
    "                item = json_data[0]  # 첫 번째 항목 사용\n",
    "                \n",
    "                result = {\n",
    "                    'file_name': f\"{row['han_text']}/{jpg_file}\",\n",
    "                    # 'image_path': jpg_path,\n",
    "                    'text': row['text'],\n",
    "                    'name': item.get('Name', ''),\n",
    "                    'Cat 1': item.get('Cat 1', ''),\n",
    "                    'Cat 2': item.get('Cat 2', ''),\n",
    "                    'Cat 3': item.get('Cat 3', ''),\n",
    "                    'Cat 4': item.get('Cat 4', '')\n",
    "                }\n",
    "                results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 jsonl 파일 경로\n",
    "output_jsonl = 'metadata.jsonl'\n",
    "\n",
    "# jsonl 파일에 결과 저장\n",
    "with open(output_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing and saving files\"):\n",
    "        results = process_files(row)\n",
    "        for result in results:\n",
    "            json.dump(result, f, ensure_ascii=False)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load_dataset('image_folder'): 실패\n",
    "\n",
    "너무 오래 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('imagefolder', data_dir='/home/work/llm_data/datasets/food-images/', streaming=True, logger=logging.getLogger())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebDataSet 으로 시도\n",
    "\n",
    "2,373,670 건의 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_webdataset(jsonl_path, image_root, output_path, samples_per_shard=20000):\n",
    "    writer = wds.TarWriter(output_path)\n",
    "    sample_count = 0\n",
    "    shard_count = 0\n",
    "\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line)\n",
    "            image_path = os.path.join(image_root, data['file_name'])\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Warning: Image not found - {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # 이미지를 바이트로 읽기\n",
    "            with open(image_path, 'rb') as img_file:\n",
    "                image_bytes = img_file.read()\n",
    "\n",
    "            # WebDataset 샘플 생성\n",
    "            sample = {\n",
    "                \"__key__\": f\"sample_{sample_count}\",\n",
    "                \"jpg\": image_bytes,\n",
    "                \"json\": json.dumps(data)\n",
    "            }\n",
    "            writer.write(sample)\n",
    "\n",
    "            sample_count += 1\n",
    "\n",
    "            # 새 샤드 시작\n",
    "            if sample_count % samples_per_shard == 0:\n",
    "                writer.close()\n",
    "                shard_count += 1\n",
    "                writer = wds.TarWriter(f\"{output_path}_{shard_count:05d}.tar\")\n",
    "                # break\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Created {shard_count + 1} shards with {sample_count} samples in total.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_path = '/home/work/llm_data/datasets/food-images/metadata.jsonl'\n",
    "image_root = '/home/work/llm_data/datasets/food-images/Training'\n",
    "output_path = '/home/work/llm_data/datasets/food-images/webdataset/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_webdataset(jsonl_path, image_root, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39be62cb9ad446882c12c3adf112265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"webdataset\", data_dir='/home/work/llm_data/datasets/food-images/webdataset', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__key__': 'sample_1',\n",
       " '__url__': '/home/work/llm_data/datasets/food-images/webdataset/data',\n",
       " 'jpg': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2700x1800>,\n",
       " 'json': {'Cat 1': '26',\n",
       "  'Cat 2': '01',\n",
       "  'Cat 3': '57',\n",
       "  'Cat 4': 'xx',\n",
       "  'file_name': '파프리카빨강/A260157XX_02686.jpg',\n",
       "  'name': 'papeulika,ppalgang',\n",
       "  'text': 'Red bell pepper'},\n",
       " 'text': 'Red bell pepper'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(example):\n",
    "    example['text'] = example['json']['text']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(extract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'__key__': 'sample_0', '__url__': '/home/work/llm_data/datasets/food-images/webdataset/data', 'jpg': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1748x1309 at 0x7FDED7617760>, 'json': {'Cat 1': '03', 'Cat 2': '02', 'Cat 3': '02', 'Cat 4': 'xx', 'file_name': '볶음쌀국수/B030202XX_31447.jpg', 'name': 'bokk-eumssalgugsu', 'text': 'Stir-fried rice noodles'}, 'text': 'Stir-fried rice noodles'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].column_names)\n",
    "print(next(iter(dataset['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    n_shards: 119\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
